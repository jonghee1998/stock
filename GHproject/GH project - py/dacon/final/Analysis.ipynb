{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.regularizers import L1L2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Technical Analysis - LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'aapl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1. Load data and Remove Missing value\n",
    "df = pd.read_csv(f'dacon/final/Loaded data/{ticker}_stock_Tech_data.csv')\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "df = df.dropna()\n",
    "df.isnull().sum() \n",
    "\n",
    "\n",
    "## 1.2. Apply MinMax Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scale_cols = ['Open', 'High', 'Low', 'Close','Adj Close',\n",
    "              'EMA5','EMA20','EMA60','EMA120','MA5','MA20','MA60','MA120',\n",
    "              'BOL_H1', 'BOL_AVG', 'BOL_L1','BOL_H2','BOL_L2'\n",
    "              ]\n",
    "scaled_df = scaler.fit_transform(df[scale_cols])\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=scale_cols) \n",
    "\n",
    "# Define Input Parameter: feature, label => numpy type\n",
    "def make_sequene_dataset(feature, label, window_size):\n",
    "    feature_list = []      \n",
    "    label_list = []        \n",
    "    for i in range(len(feature)-window_size):\n",
    "        feature_list.append(feature[i:i+window_size]) # 1-window size까지 feature에 추가 ... 를 반복\n",
    "        label_list.append(label[i+window_size]) # window size + 1 번째는 label에 추가 ... 를 반복\n",
    "    return np.array(feature_list), np.array(label_list) \n",
    "\n",
    "# feature_df, label_df 생성\n",
    "feature_cols = ['Open', 'High', 'Low', 'Close','MA5','MA20','MA60','MA120',\n",
    "              'EMA5','EMA20','EMA60','EMA120',\n",
    "              'BOL_H1', 'BOL_AVG', 'BOL_L1','BOL_H2','BOL_L2']\n",
    "label_cols = [ 'Adj Close' ]\n",
    "\n",
    "feature_df = pd.DataFrame(scaled_df, columns=feature_cols)\n",
    "label_df = pd.DataFrame(scaled_df, columns=label_cols)\n",
    "\n",
    "# DataFrame => Numpy 변환\n",
    "feature_np = feature_df.to_numpy()\n",
    "label_np = label_df.to_numpy()\n",
    "\n",
    "print(feature_np.shape, label_np.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Create data    \n",
    "# 3.1. Set window size\n",
    "window_size = 30\n",
    "X, Y = make_sequene_dataset(feature_np, label_np, window_size)\n",
    "print(X.shape, Y.shape) # (2452, 50, 5) (2452, 1)\n",
    "\n",
    "# 3.2. Split into train, test (split = int(len(X)*0.95))\n",
    "split = int(len(X)*0.80) \n",
    "x_train = X[0:split]\n",
    "y_train = Y[0:split]\n",
    "\n",
    "x_test = X[split:]\n",
    "y_test = Y[split:]\n",
    "\n",
    "print(x_train.shape, y_train.shape) # (1961, 50, 5) (1961, 1)\n",
    "print(x_test.shape, y_test.shape) # (491, 50, 5) (491, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Construct and Compile model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, activation='tanh', input_shape=x_train[0].shape, return_sequences=False, \n",
    "               kernel_regularizer=L1L2(l1=0.0001, l2=0.0001), recurrent_regularizer=L1L2(l1=0.0001, l2=0.0001)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(LSTM(64, activation='tanh'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# 모델 학습 과정에서의 손실(loss) 값을 기록하기 위한 리스트\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "# model 학습 (checkpoint, earlystopping, reduceLR 적용)\n",
    "save_best_only=tf.keras.callbacks.ModelCheckpoint(filepath=\"jonghee_test/tech lstm_model.h5\", monitor='val_loss', save_best_only=True) #가장 좋은 성능을 낸 val_loss가 적은 model만 남겨 놓았습니다.\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "#reduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10) #검증 손실이 10epoch 동안 좋아지지 않으면 학습률을 0.1 배로 재구성하는 명령어입니다.\n",
    "\n",
    "hist = model.fit(x_train, y_train, \n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=100, batch_size=150,        # 100번 학습 - loss가 점점 작아진다, 만약 100번의 학습을 다 하지 않더라도 loss 가 더 줄지 않는다면, 맞춰둔 조건에 따라 조기종료가 이루어진다\n",
    "          callbacks=[early_stop]) # save_best_only ,\n",
    "\n",
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical model Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가지표 1: 예측 그래프\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('Technical analysis Prediction model')\n",
    "plt.ylabel('Adj Close')\n",
    "plt.xlabel('period')\n",
    "plt.plot(y_test, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 평가지표 2: 학습곡선\n",
    "train_loss_history.extend(hist.history['loss']) # 학습 과정에서의 손실값(로스) 기록\n",
    "val_loss_history.extend(hist.history['val_loss'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 평가지표 3: MAPE, MAE, RMSE\n",
    "mape = np.sum(abs(y_test - pred) / y_test) / len(x_test)\n",
    "mae = np.mean(np.abs(y_test - pred))\n",
    "rmse = np.sqrt(np.mean(np.square(y_test - pred)))\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metrics': ['MAPE', 'MAE', 'RMSE'],\n",
    "    'Values': [mape, mae, rmse]})\n",
    "\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking 모델의 인풋으로 사용하기 위한 데이터프레임화 + 실제 다음날 수정종가 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_df = pd.DataFrame(np.zeros((len(y_test), len(scale_cols))), columns=scale_cols) # y_test 역변환을 위한 임시 DataFrame\n",
    "inverse_df['Adj Close'] = y_test.flatten()\n",
    "real_y_test = scaler.inverse_transform(inverse_df)[:, inverse_df.columns.get_loc('Adj Close')]\n",
    "\n",
    "inverse_df['Adj Close'] = pred.flatten() # pred 역변환을 위한 임시 DataFrame\n",
    "real_pred = scaler.inverse_transform(inverse_df)[:, inverse_df.columns.get_loc('Adj Close')]\n",
    "\n",
    "dates = df.index[split+window_size:].values # 해당 날짜 가져오기\n",
    "\n",
    "tech_result_df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Real Price': real_y_test,\n",
    "    'Predicted Price': real_pred\n",
    "})\n",
    "\n",
    "## 진짜 예측값 추출하기\n",
    "\n",
    "last_date = pd.to_datetime(df.index[-1]) # df의 마지막 행의 날짜를 가져옴\n",
    "\n",
    "if last_date.weekday() == 4:  # 0: 월요일, 1: 화요일, ..., 4: 금요일\n",
    "    next_day = last_date + pd.Timedelta(days=3)  # 금요일에서 3일 후는 월요일\n",
    "else:\n",
    "    next_day = last_date + pd.Timedelta(days=1)  # 그 외의 경우에는 하루를 더함\n",
    "\n",
    "\n",
    "# 1. Extract the last 50 days data\n",
    "recent_feature = feature_np[-window_size:]\n",
    "recent_feature = recent_feature.reshape(1, window_size, -1)\n",
    "\n",
    "# 2. Predict the value for the next day\n",
    "predicted_new = model.predict(recent_feature)\n",
    "\n",
    "# 3. Inverse transform the predicted value to its original scale\n",
    "dummy_data = np.zeros((1, scaled_df.shape[1] - 1))\n",
    "predicted_new_full_features = np.hstack([predicted_new, dummy_data])\n",
    "\n",
    "tech_predicted_new_original = scaler.inverse_transform(predicted_new_full_features)[0, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fundamental Analysis - LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1. Load data\n",
    "df = pd.read_csv(f'dacon/final/Loaded data/{ticker}_FS_summary.csv')\n",
    "df2 = pd.read_csv(f'dacon/final/Loaded data/Econ_data.csv')\n",
    "df3 = pd.read_csv(f'dacon/final/Loaded data/Industry_data.csv')\n",
    "\n",
    "\n",
    "df.set_index('Date', inplace=True)\n",
    "df2.set_index('Date', inplace=True)\n",
    "df3.set_index('Date', inplace=True)\n",
    "\n",
    "\n",
    "# 1.2. Select the required columns from df2\n",
    "df2_selected = df2[['GDP', 'CPIAUCSL']].copy()\n",
    "df3_selected = df3[['NDAQ Adj Close', 'DJI Adj Close', 'SPX Adj Close'] + [df3.columns[-2]]]\n",
    "\n",
    "# 1.3. Merge the selected data with df based on the Date\n",
    "df = df.merge(df2_selected, on='Date', how='left')\n",
    "df = df.merge(df3_selected, on='Date', how='left')\n",
    "\n",
    "## 2.1. Remove Outliers & Missing value\n",
    "df = df.dropna()\n",
    "df.isnull().sum() \n",
    "\n",
    "## 2.2. Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scale_cols = df.columns.tolist()\n",
    "scaled_df = scaler.fit_transform(df[scale_cols])\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=scale_cols) \n",
    "\n",
    "# Define Input Parameter: feature, label => numpy type\n",
    "def make_sequene_dataset(feature, label, window_size):\n",
    "    feature_list = []      \n",
    "    label_list = []        \n",
    "    for i in range(len(feature)-window_size):\n",
    "        feature_list.append(feature[i:i+window_size]) # 1-window size까지 feature에 추가 ... 를 반복\n",
    "        label_list.append(label[i+window_size]) # window size + 1 번째는 label에 추가 ... 를 반복\n",
    "    return np.array(feature_list), np.array(label_list) \n",
    "\n",
    "# feature_df, label_df 생성\n",
    "feature_cols = df.columns.drop('Adj Close').tolist()\n",
    "label_cols = [ 'Adj Close' ]\n",
    "\n",
    "feature_df = pd.DataFrame(scaled_df, columns=feature_cols)\n",
    "label_df = pd.DataFrame(scaled_df, columns=label_cols)\n",
    "\n",
    "# DataFrame => Numpy 변환\n",
    "feature_np = feature_df.to_numpy()\n",
    "label_np = label_df.to_numpy()\n",
    "\n",
    "print(feature_np.shape, label_np.shape) # (2353, 16) (2353, 1)\n",
    "\n",
    "   \n",
    "## 3. Create data    \n",
    "# 3.1. Set window size\n",
    "window_size = 30\n",
    "X, Y = make_sequene_dataset(feature_np, label_np, window_size)\n",
    "print(X.shape, Y.shape) # (2452, 50, 5) (2452, 1)\n",
    "\n",
    "# 3.2. Split into train, test (split = int(len(X)*0.95))\n",
    "split = int(len(X)*0.80) \n",
    "x_train = X[0:split]\n",
    "y_train = Y[0:split]\n",
    "\n",
    "x_test = X[split:]\n",
    "y_test = Y[split:]\n",
    "\n",
    "print(x_train.shape, y_train.shape) # (1961, 50, 5) (1961, 1)\n",
    "print(x_test.shape, y_test.shape) # (491, 50, 5) (491, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, activation='tanh', input_shape=x_train[0].shape, return_sequences=True,\n",
    "               kernel_regularizer=L1L2(l1=0.001, l2=0.001), recurrent_regularizer=L1L2(l1=0.001, l2=0.001)))\n",
    "               \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(64, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# 모델 학습 과정에서의 손실(loss) 값을 기록하기 위한 리스트\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "# model 학습 (checkpoint, earlystopping, reduceLR 적용)\n",
    "save_best_only=tf.keras.callbacks.ModelCheckpoint(filepath=\"jonghee_test/tech lstm_model.h5\", monitor='val_loss', save_best_only=True) #가장 좋은 성능을 낸 val_loss가 적은 model만 남겨 놓았습니다.\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "#reduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10) #검증 손실이 10epoch 동안 좋아지지 않으면 학습률을 0.1 배로 재구성하는 명령어입니다.\n",
    "\n",
    "hist = model.fit(x_train, y_train, \n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=100, batch_size=150,        # 100번 학습 - loss가 점점 작아진다, 만약 100번의 학습을 다 하지 않더라도 loss 가 더 줄지 않는다면, 맞춰둔 조건에 따라 조기종료가 이루어진다\n",
    "          callbacks=[early_stop]) # save_best_only ,\n",
    "\n",
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental model Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가지표 1: 예측 그래프\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('Fundamental analysis Prediction model')\n",
    "plt.ylabel('Close')\n",
    "plt.xlabel('period')\n",
    "plt.plot(y_test, label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 평가지표 2: 학습곡선\n",
    "train_loss_history.extend(hist.history['loss']) # 학습 과정에서의 손실값(로스) 기록\n",
    "val_loss_history.extend(hist.history['val_loss'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 평가지표 3: MAPE, MAE, RMSE\n",
    "mape = np.sum(abs(y_test - pred) / y_test) / len(x_test)\n",
    "mae = np.mean(np.abs(y_test - pred))\n",
    "rmse = np.sqrt(np.mean(np.square(y_test - pred)))\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metrics': ['MAPE', 'MAE', 'RMSE'],\n",
    "    'Values': [mape, mae, rmse]})\n",
    "\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking 모델의 인풋으로 사용하기 위한 데이터프레임화 + 실제 다음날 수정종가 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_df = pd.DataFrame(np.zeros((len(y_test), len(scale_cols))), columns=scale_cols) # y_test 역변환을 위한 임시 DataFrame\n",
    "inverse_df['Adj Close'] = y_test.flatten()\n",
    "real_y_test = scaler.inverse_transform(inverse_df)[:, inverse_df.columns.get_loc('Adj Close')]\n",
    "\n",
    "inverse_df['Adj Close'] = pred.flatten() # pred 역변환을 위한 임시 DataFrame\n",
    "real_pred = scaler.inverse_transform(inverse_df)[:, inverse_df.columns.get_loc('Adj Close')]\n",
    "\n",
    "dates = df.index[split+window_size:].values # 해당 날짜 가져오기\n",
    "\n",
    "fund_result_df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Real Price': real_y_test,\n",
    "    'Predicted Price': real_pred\n",
    "})\n",
    "\n",
    "## 진짜 예측값 추출하기\n",
    "\n",
    "last_date = pd.to_datetime(df.index[-1]) # df의 마지막 행의 날짜를 가져옴\n",
    "\n",
    "if last_date.weekday() == 4:  # 0: 월요일, 1: 화요일, ..., 4: 금요일\n",
    "    next_day = last_date + pd.Timedelta(days=3)  # 금요일에서 3일 후는 월요일\n",
    "else:\n",
    "    next_day = last_date + pd.Timedelta(days=1)  # 그 외의 경우에는 하루를 더함\n",
    "\n",
    "\n",
    "# 1. Extract the last 50 days data\n",
    "latest_data = feature_np[-window_size:]\n",
    "latest_data = latest_data.reshape(1, window_size, -1)\n",
    "\n",
    "# 2. Predict the value for the next day\n",
    "next_day_pred = model.predict(latest_data)\n",
    "\n",
    "# 3. Inverse transform the predicted value to its original scale\n",
    "inverse_df_temp = pd.DataFrame(np.zeros((1, len(scale_cols))), columns=scale_cols)  # 역변환을 위한 임시 DataFrame\n",
    "inverse_df_temp['Adj Close'] = next_day_pred.flatten()\n",
    "fund_predicted_new_original = scaler.inverse_transform(inverse_df_temp)[:, inverse_df_temp.columns.get_loc('Adj Close')]\n",
    "\n",
    "# Convert numpy array value to scalar\n",
    "fund_predicted_new_original = fund_predicted_new_original.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. XGBoost를 이용한 Stacking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(tech_result_df[['Date', 'Real Price', 'Predicted Price']], \n",
    "                     fund_result_df[['Date', 'Predicted Price']],\n",
    "                     on='Date', \n",
    "                     how='inner', \n",
    "                     suffixes=('_Tech', '_Fund'))\n",
    "\n",
    "df.columns = ['Date', 'Real Price', 'Tech_Pred', 'Fund_Pred'] # Rename Column\n",
    "df = df.set_index('Date').sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax\n",
    "scaler = MinMaxScaler()\n",
    "scale_cols = ['Real Price', 'Tech_Pred', 'Fund_Pred']\n",
    "scaled_df = scaler.fit_transform(df[scale_cols])\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=scale_cols) \n",
    "\n",
    "print(scaled_df)\n",
    "\n",
    "# 2. Create Feature/Label for Stacking model\n",
    "X_stack = scaled_df[['Tech_Pred', 'Fund_Pred']].values\n",
    "y_stack = scaled_df['Real Price'].values\n",
    "\n",
    "# Data split (20% test)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_stack, y_stack, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Meta model training using XGBoost with GridSearchCV\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9],\n",
    "    'n_estimators': [50, 100],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0.5, 1, 1.5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Train the model with best parameters\n",
    "meta_model_xgb = xgb.XGBRegressor(objective='reg:squarederror', **best_params)\n",
    "meta_model_xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = meta_model_xgb.predict(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비주얼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일링된 데이터에서 예측값 추출\n",
    "y_val_original = scaler.inverse_transform(np.column_stack([y_val, np.zeros_like(y_val), np.zeros_like(y_val)]))[:, 0]\n",
    "y_pred_original = scaler.inverse_transform(np.column_stack([y_pred, np.zeros_like(y_pred), np.zeros_like(y_pred)]))[:, 0]\n",
    "tech_pred_original = scaler.inverse_transform(np.column_stack([np.zeros_like(y_pred), X_val[:, 0], np.zeros_like(y_pred)]))[:, 1]\n",
    "fund_pred_original = scaler.inverse_transform(np.column_stack([np.zeros_like(y_pred), np.zeros_like(y_pred), X_val[:, 1]]))[:, 2]\n",
    "\n",
    "# 날짜 데이터 추출\n",
    "date_train, date_val = train_test_split(df.index, test_size=0.2, random_state=42)\n",
    "\n",
    "# 그래프 그리기 준비\n",
    "plot_df = pd.DataFrame({\n",
    "    'Date': date_val,\n",
    "    'Real Price': y_val_original,\n",
    "    'Meta Predicted Price': y_pred_original,\n",
    "    'Stock Predicted Price': tech_pred_original,\n",
    "    'FS Predicted Price': fund_pred_original\n",
    "})\n",
    "\n",
    "# 날짜를 기준으로 정렬\n",
    "plot_df = plot_df.sort_values(by='Date')\n",
    "plot_df['Date'] = pd.to_datetime(plot_df['Date'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(plot_df['Date'], plot_df['Real Price'], label='Real Price', linewidth=2)\n",
    "plt.plot(plot_df['Date'], plot_df['Meta Predicted Price'], label='Meta Predicted Price', linewidth=1.5)\n",
    "plt.plot(plot_df['Date'], plot_df['Stock Predicted Price'], '--', label='Stock Predicted Price', linewidth=1.5)\n",
    "plt.plot(plot_df['Date'], plot_df['FS Predicted Price'], '--', label='FS Predicted Price', linewidth=1.5)\n",
    "\n",
    "ax = plt.gca() # X축의 라벨 간격 설정\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))  # 1개월 간격으로 설정\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # 날짜 포맷 설정\n",
    "\n",
    "plt.title(\"Prediction vs Actual Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)  # X축 라벨 45도로 기울이기\n",
    "plt.tight_layout()  # 그래프가 잘 보이도록 레이아웃 조정\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정확도 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MSE 비교 그래프\n",
    "y_train_pred = meta_model_xgb.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "val_mse = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "plt.bar(['Train MSE', 'Validation MSE'], [train_mse, val_mse], color=['blue', 'red'])\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Train vs Validation MSE')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2. 교차 검증\n",
    "cross_val_mse = -cross_val_score(meta_model_xgb, X_stack, y_stack, cv=10, scoring='neg_mean_squared_error').mean()\n",
    "errors = [val_mse, cross_val_mse]\n",
    "labels = ['Test MSE', 'Cross Validation MSE']\n",
    "\n",
    "plt.bar(labels, errors, color=['blue', 'red'])\n",
    "plt.ylabel('MSE Value')\n",
    "plt.title('Comparison of MSE values, fold = 10')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "## 3. 학습 곡선 (임의)\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve( # 학습 곡선 데이터 얻기\n",
    "    meta_model_xgb, X_stack, y_stack, cv=5,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "\n",
    "train_scores_mean = -train_scores.mean(axis=1) # 평균 및 표준 편차 계산\n",
    "train_scores_std = train_scores.std(axis=1)\n",
    "val_scores_mean = -val_scores.mean(axis=1)\n",
    "val_scores_std = val_scores.std(axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, \n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                 val_scores_mean + val_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, val_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Learning Curve for Meta Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제 다음날 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tech와 Fund 모델로부터 얻은 다음 날의 예측 값을 표현하자면 다음과 같습니다:\n",
    "next_day_tech_pred = tech_predicted_new_original  # 여기에 Tech 모델로부터 얻은 다음 날 예측 값을 넣어주세요.\n",
    "next_day_fund_pred = fund_predicted_new_original  # 여기에 Fund 모델로부터 얻은 다음 날 예측 값을 넣어주세요.\n",
    "\n",
    "# 이 값을 스케일링 합니다:\n",
    "next_day_tech_pred_scaled = scaler.transform([[0, next_day_tech_pred, 0]])[0][1]  # 가격 부분만 스케일링\n",
    "next_day_fund_pred_scaled = scaler.transform([[0, 0, next_day_fund_pred]])[0][2]  # 가격 부분만 스케일링\n",
    "\n",
    "# 스택킹 모델을 사용하여 예측합니다:\n",
    "next_day_meta_pred_scaled = meta_model_xgb.predict(np.array([[next_day_tech_pred_scaled, next_day_fund_pred_scaled]]))[0]\n",
    "\n",
    "# 예측 값을 원래의 스케일로 변환합니다:\n",
    "next_day_meta_pred = scaler.inverse_transform([[next_day_meta_pred_scaled, 0, 0]])[0][0]\n",
    "\n",
    "last_date = pd.to_datetime(df.index[-1]) # df의 마지막 행의 날짜를 가져옴\n",
    "\n",
    "if last_date.weekday() == 4:  # 0: 월요일, 1: 화요일, ..., 4: 금요일\n",
    "    next_day = last_date + pd.Timedelta(days=3)  # 금요일에서 3일 후는 월요일\n",
    "else:\n",
    "    next_day = last_date + pd.Timedelta(days=1)  # 그 외의 경우에는 하루를 더함\n",
    "\n",
    "print(f\"다음 날({next_day.strftime('%Y-%m-%d')})의 Tech 예측 가격은: {tech_predicted_new_original}\") \n",
    "print(f\"다음 날({next_day.strftime('%Y-%m-%d')})의 Funda 예측 가격은: {fund_predicted_new_original}\") \n",
    "print(f\"다음 날({next_day.strftime('%Y-%m-%d')})의 최종 예측 가격은: {next_day_meta_pred}\") "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
